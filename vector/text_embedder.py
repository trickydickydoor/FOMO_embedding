import json
import os
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
import re
from .text_splitter import TextSplitter

try:
    from google import genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("è­¦å‘Š: Google Generative AIå®¢æˆ·ç«¯æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install google-generativeai")

class TextEmbedder:
    """å¤„ç†æ–‡æœ¬embeddingçš„ç”Ÿæˆ - ä½¿ç”¨Google Gemini API"""
    
    def __init__(self, config_file: str = 'embedding_config.json', log_callback: Optional[callable] = None):
        """
        åˆå§‹åŒ–æ–‡æœ¬embeddingå¤„ç†å™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
        """
        self.config_file = config_file
        self.log_callback = log_callback or print
        self.client = None
        self.model_name = "models/embedding-001"
        self.batch_size = 100
        
        # æ–‡æœ¬å¤„ç†è®¾ç½®
        self.max_content_length = 2000
        self.enable_text_splitting = True
        self.chunk_size = 1000
        self.chunk_overlap = 200
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        if self.enable_text_splitting:
            self.text_splitter = TextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½é…ç½®å¹¶åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._load_config()
    
    def _load_config(self) -> bool:
        """åŠ è½½embeddingé…ç½®"""
        if not GEMINI_AVAILABLE:
            self.log_callback("é”™è¯¯: Google Generative AIå®¢æˆ·ç«¯æœªå®‰è£…")
            return False
            
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„é…ç½®æ–‡ä»¶è·¯å¾„
            possible_paths = [
                self.config_file,
                os.path.join(os.path.dirname(os.path.abspath(__file__)), self.config_file),
                os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), self.config_file),
                os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'vector', self.config_file)
            ]
            
            config_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    config_path = path
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
            if not config_path:
                return self._load_from_env()
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # è·å–Geminié…ç½®
            gemini_config = config.get('gemini', {})
            api_key = gemini_config.get('api_key', '').strip()
            self.model_name = gemini_config.get('model_name', 'models/embedding-001')
            self.batch_size = gemini_config.get('batch_size', 100)
            
            # è·å–æ–‡æœ¬å¤„ç†è®¾ç½®
            embedding_settings = config.get('embedding_settings', {})
            text_prep = embedding_settings.get('text_preparation', {})
            self.max_content_length = text_prep.get('max_content_length', 2000)
            self.enable_text_splitting = text_prep.get('enable_text_splitting', True)
            self.chunk_size = text_prep.get('chunk_size', 1000)
            self.chunk_overlap = text_prep.get('chunk_overlap', 200)
            
            # é‡æ–°åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
            if self.enable_text_splitting:
                self.text_splitter = TextSplitter(
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap
                )
            
            if not api_key:
                self.log_callback("è­¦å‘Š: Gemini API Key ä¸ºç©º")
                return False
            
            # åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯
            self.client = genai.Client(api_key=api_key)
            
            self.log_callback(f"Gemini API å·²è¿æ¥ï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
            return True
            
        except json.JSONDecodeError as e:
            self.log_callback(f"è­¦å‘Š: é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            return False
        except Exception as e:
            self.log_callback(f"è­¦å‘Š: åŠ è½½ Gemini é…ç½®å¤±è´¥: {e}")
            return False
    
    def _load_from_env(self) -> bool:
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        try:
            api_key = os.getenv('GEMINI_API_KEY', '').strip()
            self.model_name = os.getenv('GEMINI_MODEL', 'models/embedding-001')
            self.batch_size = int(os.getenv('GEMINI_BATCH_SIZE', '100'))
            
            # ä»ç¯å¢ƒå˜é‡è·å–æ–‡æœ¬å¤„ç†è®¾ç½®
            self.enable_text_splitting = os.getenv('ENABLE_TEXT_SPLITTING', 'true').lower() == 'true'
            self.chunk_size = int(os.getenv('CHUNK_SIZE', '1000'))
            self.chunk_overlap = int(os.getenv('CHUNK_OVERLAP', '200'))
            
            # é‡æ–°åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
            if self.enable_text_splitting:
                self.text_splitter = TextSplitter(
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap
                )
            
            if not api_key:
                self.log_callback("è­¦å‘Š: æœªæ‰¾åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡")
                return False
            
            # åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯
            self.client = genai.Client(api_key=api_key)
            
            self.log_callback(f"Gemini API å·²ä»ç¯å¢ƒå˜é‡è¿æ¥ï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
            return True
            
        except Exception as e:
            self.log_callback(f"ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®å¤±è´¥: {e}")
            return False
    
    def _clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤ä¸å¿…è¦çš„å­—ç¬¦å’Œæ ¼å¼
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        
        return text.strip()
    
    def _prepare_text_for_embedding(self, news_item: Dict[str, Any]) -> str:
        """
        ä¸ºembeddingå‡†å¤‡æ–‡æœ¬å†…å®¹ - åªä½¿ç”¨contentå­—æ®µ
        
        Args:
            news_item: æ–°é—»é¡¹æ•°æ®
            
        Returns:
            å‡†å¤‡å¥½çš„æ–‡æœ¬
        """
        content = self._clean_text(news_item.get('content', ''))
        
        # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        if not content:
            return ""
        
        # é™åˆ¶å†…å®¹é•¿åº¦
        if len(content) > self.max_content_length:
            content = content[:self.max_content_length] + "..."
        
        return content
    
    def _split_into_batches(self, items: List[Any], batch_size: int) -> List[List[Any]]:
        """
        å°†åˆ—è¡¨åˆ†å‰²æˆæ‰¹æ¬¡
        
        Args:
            items: è¦åˆ†å‰²çš„åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            æ‰¹æ¬¡åˆ—è¡¨
        """
        batches = []
        for i in range(0, len(items), batch_size):
            batches.append(items[i:i + batch_size])
        return batches
    
    def generate_embeddings(self, news_items: List[Dict[str, Any]], 
                          retry_attempts: int = 3) -> Tuple[List[List[float]], List[Dict[str, Any]]]:
        """
        ä¸ºæ–°é—»é¡¹ç”Ÿæˆembeddingsï¼ˆæ”¯æŒæ–‡æœ¬åˆ†å‰²ï¼‰
        
        Args:
            news_items: æ–°é—»é¡¹åˆ—è¡¨
            retry_attempts: é‡è¯•æ¬¡æ•°
            
        Returns:
            (embeddingsåˆ—è¡¨, å¯¹åº”çš„chunkä¿¡æ¯åˆ—è¡¨)
        """
        if not self.client or not news_items:
            return [], []
        
        # å‡†å¤‡æ–‡æœ¬å—
        all_chunks = []
        for news_idx, item in enumerate(news_items):
            content = self._clean_text(item.get('content', ''))
            
            if not content:
                continue
                
            if self.enable_text_splitting:
                # åˆ†å‰²æ–‡æœ¬
                chunks = self.text_splitter.split_text(content)
                
                for chunk_idx, chunk_info in enumerate(chunks):
                    all_chunks.append({
                        'text': chunk_info['text'],
                        'news_item': item,
                        'news_index': news_idx,
                        'chunk_index': chunk_idx,
                        'chunk_id': f"{item.get('id', 'unknown')}_{chunk_idx}",
                        'line_start': chunk_info['line_start'],
                        'line_end': chunk_info['line_end'],
                        'char_start': chunk_info['char_start'],
                        'char_end': chunk_info['char_end']
                    })
            else:
                # ä¸åˆ†å‰²ï¼Œæ•´ç¯‡å¤„ç†
                if len(content) > self.max_content_length:
                    content = content[:self.max_content_length] + "..."
                
                all_chunks.append({
                    'text': content,
                    'news_item': item,
                    'news_index': news_idx,
                    'chunk_index': 0,
                    'chunk_id': f"{item.get('id', 'unknown')}_full",
                    'line_start': 1,
                    'line_end': content.count('\n') + 1,
                    'char_start': 0,
                    'char_end': len(content)
                })
        
        if not all_chunks:
            self.log_callback("æ²¡æœ‰å¯å¤„ç†çš„æ–‡æœ¬å—")
            return [], []
        
        # æå–è¦å‘é‡åŒ–çš„æ–‡æœ¬
        texts_to_embed = [chunk['text'] for chunk in all_chunks]
        
        # åˆ†æ‰¹å¤„ç†
        batches = self._split_into_batches(texts_to_embed, self.batch_size)
        all_embeddings = []
        successful_chunks = []
        
        chunk_idx = 0
        for batch_idx, batch_texts in enumerate(batches):
            self.log_callback(f"å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)} ({len(batch_texts)} ä¸ªæ–‡æœ¬å—)")
            
            for attempt in range(retry_attempts):
                try:
                    # è°ƒç”¨Gemini API
                    embeddings_batch = []
                    for text in batch_texts:
                        result = self.client.models.embed_content(
                            model=self.model_name,
                            contents=text
                        )
                        embeddings_batch.append(result.embeddings[0].values)
                    
                    # è®°å½•æˆåŠŸçš„ç»“æœ
                    for i, embedding in enumerate(embeddings_batch):
                        all_embeddings.append(embedding)
                        successful_chunks.append(all_chunks[chunk_idx + i])
                    
                    chunk_idx += len(batch_texts)
                    
                    self.log_callback(f"  âœ… æ‰¹æ¬¡ {batch_idx + 1} æˆåŠŸç”Ÿæˆ {len(embeddings_batch)} ä¸ªembeddings (768ç»´)")
                    break
                    
                except Exception as e:
                    self.log_callback(f"  âŒ æ‰¹æ¬¡ {batch_idx + 1} å¤±è´¥ (å°è¯• {attempt + 1}/{retry_attempts}): {e}")
                    
                    if attempt < retry_attempts - 1:
                        time.sleep(5 + (2 ** attempt))
                    else:
                        self.log_callback(f"  ğŸ’€ æ‰¹æ¬¡ {batch_idx + 1} æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡")
                        chunk_idx += len(batch_texts)  # è·³è¿‡å¤±è´¥çš„å—
        
        total_chunks = len(all_chunks)
        successful_count = len(all_embeddings)
        self.log_callback(f"Embeddingç”Ÿæˆå®Œæˆ: {successful_count}/{total_chunks} ä¸ªæ–‡æœ¬å—æˆåŠŸ")
        
        return all_embeddings, successful_chunks
    
    def generate_single_embedding(self, text: str, retry_attempts: int = 3) -> Optional[List[float]]:
        """
        ä¸ºå•ä¸ªæ–‡æœ¬ç”Ÿæˆembedding
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            retry_attempts: é‡è¯•æ¬¡æ•°
            
        Returns:
            embeddingå‘é‡æˆ–None
        """
        if not self.client or not text:
            return None
        
        cleaned_text = self._clean_text(text)
        
        for attempt in range(retry_attempts):
            try:
                result = self.client.models.embed_content(
                    model=self.model_name,
                    contents=cleaned_text
                )
                
                return result.embeddings[0].values
                
            except Exception as e:
                self.log_callback(f"å•ä¸ªembeddingç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}/{retry_attempts}): {e}")
                
                if attempt < retry_attempts - 1:
                    time.sleep(5 + (2 ** attempt))
        
        return None
    
    def check_connection(self) -> bool:
        """
        æ£€æŸ¥Gemini APIè¿æ¥çŠ¶æ€
        
        Returns:
            è¿æ¥æ˜¯å¦æ­£å¸¸
        """
        try:
            if not self.client:
                return False
            
            # æµ‹è¯•ç”Ÿæˆä¸€ä¸ªç®€å•çš„embedding
            test_embedding = self.generate_single_embedding("æµ‹è¯•è¿æ¥", retry_attempts=1)
            return test_embedding is not None and len(test_embedding) == 768
            
        except Exception as e:
            self.log_callback(f"è¿æ¥æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def estimate_cost(self, news_items: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ä¼°ç®—embeddingçš„æˆæœ¬
        
        Args:
            news_items: æ–°é—»é¡¹åˆ—è¡¨
            
        Returns:
            æˆæœ¬ä¼°ç®—ä¿¡æ¯
        """
        if not news_items:
            return {'total_tokens': 0, 'estimated_cost_usd': 0}
        
        # ä¼°ç®—tokenæ•°é‡ï¼ˆ1ä¸ªä¸­æ–‡å­—ç¬¦çº¦ç­‰äº1.5ä¸ªtokenï¼‰
        total_chars = 0
        for item in news_items:
            text = self._prepare_text_for_embedding(item)
            total_chars += len(text)
        
        # ä¼°ç®—tokenæ•°é‡
        estimated_tokens = int(total_chars * 1.5)
        
        # Gemini embeddingçš„ä»·æ ¼ (éœ€è¦ç¡®è®¤å®é™…ä»·æ ¼)
        price_per_1k_tokens = 0.00001
        estimated_cost = (estimated_tokens / 1000) * price_per_1k_tokens
        
        return {
            'total_items': len(news_items),
            'total_characters': total_chars,
            'estimated_tokens': estimated_tokens,
            'estimated_cost_usd': round(estimated_cost, 6),
            'model': self.model_name,
            'dimensions': 768,
            'provider': 'Google Gemini API',
            'note': 'æ¯åˆ†é’Ÿ15ä¸ªè¯·æ±‚å…è´¹é¢åº¦'
        }